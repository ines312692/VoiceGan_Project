{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook 3: Entraînement du Modèle\n",
    "\n",
    "Ce notebook démontre l'entraînement du modèle VoiceGAN.\n",
    "\n",
    "**Objectifs:**\n",
    "- Tester le modèle sur des données synthétiques\n",
    "- Vérifier les dimensions\n",
    "- Lancer un mini-entraînement\n",
    "- Analyser les pertes\n",
    "- Générer des exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display as ipd\n",
    "from pathlib import Path\n",
    "\n",
    "from config.model_config import Config\n",
    "from src.models.voicegan import VoiceGAN\n",
    "from src.training.dataset import VoiceDataset, VoiceCollator\n",
    "from src.losses.losses import VoiceGANLoss\n",
    "from src.preprocessing.mel_spectrogram import MelSpectrogramProcessor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../config/config.yaml')\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size: {config.training.batch_size}\")\n",
    "print(f\"  Learning rate G: {config.training.learning_rate_g}\")\n",
    "print(f\"  Learning rate D: {config.training.learning_rate_d}\")\n",
    "print(f\"  N-mels: {config.audio.n_mels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test: Initialisation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer modèle\n",
    "model = VoiceGAN(\n",
    "    n_mels=config.audio.n_mels,\n",
    "    content_channels=config.content_encoder.channels,\n",
    "    transformer_dim=config.content_encoder.transformer_dim,\n",
    "    num_heads=config.content_encoder.num_heads,\n",
    "    num_transformer_layers=config.content_encoder.num_layers,\n",
    "    style_dim=config.style_encoder.style_dim\n",
    ").to(device)\n",
    "\n",
    "print(\" Model created\")\n",
    "\n",
    "# Compter paramètres\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "# Détails par composant\n",
    "print(f\"\\nComponent parameters:\")\n",
    "print(f\"  Content Encoder: {sum(p.numel() for p in model.content_encoder.parameters()):,}\")\n",
    "print(f\"  Style Encoder: {sum(p.numel() for p in model.style_encoder.parameters()):,}\")\n",
    "print(f\"  Generator: {sum(p.numel() for p in model.generator.parameters()):,}\")\n",
    "print(f\"  Discriminator: {sum(p.numel() for p in model.discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test: Forward Pass avec Données Synthétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer données synthétiques\n",
    "batch_size = 2\n",
    "time_steps = 100\n",
    "\n",
    "source_mel = torch.randn(batch_size, config.audio.n_mels, time_steps).to(device)\n",
    "target_mel = torch.randn(batch_size, config.audio.n_mels, time_steps).to(device)\n",
    "\n",
    "print(f\"Source mel shape: {source_mel.shape}\")\n",
    "print(f\"Target mel shape: {target_mel.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    results = model(source_mel, target_mel)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Generated mel: {results['generated_mel'].shape}\")\n",
    "print(f\"  Content: {results['content'].shape}\")\n",
    "print(f\"  Style: {results['style'].shape}\")\n",
    "\n",
    "print(\"\\n Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test: Discriminateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test discriminateur\n",
    "with torch.no_grad():\n",
    "    disc_outputs, disc_features = model.discriminate(results['generated_mel'])\n",
    "\n",
    "print(f\"Discriminator outputs (multi-scale):\")\n",
    "for i, output in enumerate(disc_outputs):\n",
    "    print(f\"  Scale {i}: {output.shape}\")\n",
    "\n",
    "print(f\"\\nDiscriminator features:\")\n",
    "for i, features in enumerate(disc_features):\n",
    "    print(f\"  Scale {i}: {len(features)} feature maps\")\n",
    "\n",
    "print(\"\\n Discriminator working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test: Calcul des Pertes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer criterion\n",
    "criterion = VoiceGANLoss(\n",
    "    lambda_reconstruction=config.training.lambda_reconstruction,\n",
    "    lambda_adversarial=config.training.lambda_adversarial,\n",
    "    lambda_identity=config.training.lambda_identity,\n",
    "    lambda_content=config.training.lambda_content,\n",
    "    lambda_feature_matching=config.training.lambda_feature_matching\n",
    ")\n",
    "\n",
    "# Calculer pertes\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    results = model(source_mel, target_mel)\n",
    "    fake_mel = results['generated_mel']\n",
    "    \n",
    "    # Discriminator outputs\n",
    "    disc_fake_out, disc_fake_feats = model.discriminate(fake_mel)\n",
    "    disc_real_out, disc_real_feats = model.discriminate(target_mel)\n",
    "    \n",
    "    # Re-encode\n",
    "    content_fake = model.encode_content(fake_mel)\n",
    "    style_fake = model.encode_style(fake_mel)\n",
    "    \n",
    "    # Generator loss\n",
    "    g_loss, g_loss_dict = criterion.generator_loss(\n",
    "        real_mel=target_mel,\n",
    "        fake_mel=fake_mel,\n",
    "        disc_fake_outputs=disc_fake_out,\n",
    "        disc_fake_features=disc_fake_feats,\n",
    "        disc_real_features=disc_real_feats,\n",
    "        content_source=results['content'],\n",
    "        content_fake=content_fake,\n",
    "        style_target=results['style'],\n",
    "        style_fake=style_fake\n",
    "    )\n",
    "    \n",
    "    # Discriminator loss\n",
    "    d_loss, d_loss_dict = criterion.discriminator_loss(\n",
    "        disc_real_out, disc_fake_out\n",
    "    )\n",
    "\n",
    "print(\"Generator losses:\")\n",
    "for key, value in g_loss_dict.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nDiscriminator losses:\")\n",
    "for key, value in d_loss_dict.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n Loss computation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test: Mini-Entraînement (Overfitting Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un petit batch pour overfitting test\n",
    "print(\"Creating mini dataset...\")\n",
    "\n",
    "# Créer dataset de test\n",
    "mini_dataset = VoiceDataset(\n",
    "    data_dir='../data/train',\n",
    "    audio_config=config.audio.__dict__,\n",
    "    segment_length=config.audio.segment_length,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "# Limiter à 10 samples\n",
    "mini_dataset.pairs = mini_dataset.pairs[:10]\n",
    "\n",
    "mini_loader = DataLoader(\n",
    "    mini_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=VoiceCollator()\n",
    ")\n",
    "\n",
    "print(f\"Mini dataset size: {len(mini_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizers\n",
    "optimizer_g = torch.optim.Adam(\n",
    "    list(model.content_encoder.parameters()) +\n",
    "    list(model.style_encoder.parameters()) +\n",
    "    list(model.generator.parameters()),\n",
    "    lr=config.training.learning_rate_g,\n",
    "    betas=(config.training.beta1, config.training.beta2)\n",
    ")\n",
    "\n",
    "optimizer_d = torch.optim.Adam(\n",
    "    model.discriminator.parameters(),\n",
    "    lr=config.training.learning_rate_d,\n",
    "    betas=(config.training.beta1, config.training.beta2)\n",
    ")\n",
    "\n",
    "print(\" Optimizers created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini training loop\n",
    "num_epochs = 10\n",
    "losses_history = {'g_total': [], 'g_recon': [], 'd_total': []}\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Starting mini-training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = {'g_total': [], 'g_recon': [], 'd_total': []}\n",
    "    \n",
    "    pbar = tqdm(mini_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    for source_mel, target_mel in pbar:\n",
    "        source_mel = source_mel.to(device)\n",
    "        target_mel = target_mel.to(device)\n",
    "        \n",
    "        # === Train Generator ===\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        results = model(source_mel, target_mel)\n",
    "        fake_mel = results['generated_mel']\n",
    "        \n",
    "        disc_fake_out, disc_fake_feats = model.discriminate(fake_mel)\n",
    "        disc_real_out, disc_real_feats = model.discriminate(target_mel)\n",
    "        \n",
    "        content_fake = model.encode_content(fake_mel)\n",
    "        style_fake = model.encode_style(fake_mel)\n",
    "        \n",
    "        g_loss, g_loss_dict = criterion.generator_loss(\n",
    "            real_mel=target_mel,\n",
    "            fake_mel=fake_mel,\n",
    "            disc_fake_outputs=disc_fake_out,\n",
    "            disc_fake_features=disc_fake_feats,\n",
    "            disc_real_features=disc_real_feats,\n",
    "            content_source=results['content'],\n",
    "            content_fake=content_fake,\n",
    "            style_target=results['style'],\n",
    "            style_fake=style_fake\n",
    "        )\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # === Train Discriminator ===\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        disc_real_out, _ = model.discriminate(target_mel)\n",
    "        disc_fake_out, _ = model.discriminate(fake_mel.detach())\n",
    "        \n",
    "        d_loss, d_loss_dict = criterion.discriminator_loss(\n",
    "            disc_real_out, disc_fake_out\n",
    "        )\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # Record losses\n",
    "        epoch_losses['g_total'].append(g_loss_dict['g_total'])\n",
    "        epoch_losses['g_recon'].append(g_loss_dict['g_recon'])\n",
    "        epoch_losses['d_total'].append(d_loss_dict['d_total'])\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G': f\"{g_loss_dict['g_total']:.3f}\",\n",
    "            'D': f\"{d_loss_dict['d_total']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    # Average epoch losses\n",
    "    for key in epoch_losses:\n",
    "        avg = np.mean(epoch_losses[key])\n",
    "        losses_history[key].append(avg)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: G={losses_history['g_total'][-1]:.4f}, D={losses_history['d_total'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n Mini-training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generator losses\n",
    "axes[0].plot(losses_history['g_total'], label='Total', linewidth=2)\n",
    "axes[0].plot(losses_history['g_recon'], label='Reconstruction', linewidth=2)\n",
    "axes[0].set_title('Generator Losses')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator loss\n",
    "axes[1].plot(losses_history['d_total'], label='Discriminator', color='red', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', label='Target (~0.5)')\n",
    "axes[1].set_title('Discriminator Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(f\"  Final G loss: {losses_history['g_total'][-1]:.4f}\")\n",
    "print(f\"  Final D loss: {losses_history['d_total'][-1]:.4f}\")\n",
    "print(f\"  G loss change: {losses_history['g_total'][0] - losses_history['g_total'][-1]:.4f}\")\n",
    "\n",
    "if losses_history['g_total'][-1] < losses_history['g_total'][0]:\n",
    "    print(\"   Model is learning (G loss decreasing)\")\n",
    "else:\n",
    "    print(\"   Check: G loss not decreasing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Génération d'Exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer exemples\n",
    "model.eval()\n",
    "\n",
    "# Prendre un batch\n",
    "source_mel, target_mel = next(iter(mini_loader))\n",
    "source_mel = source_mel.to(device)\n",
    "target_mel = target_mel.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    converted_mel = model.convert(source_mel, target_mel)\n",
    "\n",
    "# Visualiser\n",
    "idx = 0  # Premier échantillon du batch\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Source\n",
    "im0 = axes[0].imshow(source_mel[idx].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title('Source (A)')\n",
    "axes[0].set_ylabel('Mel Bin')\n",
    "axes[0].set_xlabel('Time')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Target\n",
    "im1 = axes[1].imshow(target_mel[idx].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title('Target (B)')\n",
    "axes[1].set_ylabel('Mel Bin')\n",
    "axes[1].set_xlabel('Time')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# Converted\n",
    "im2 = axes[2].imshow(converted_mel[idx].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2].set_title('Converted (A→B)')\n",
    "axes[2].set_ylabel('Mel Bin')\n",
    "axes[2].set_xlabel('Time')\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: After only 10 epochs, the conversion won't be perfect.\")\n",
    "print(\"This is expected. Full training requires 50-100+ epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conversion en Audio (Griffin-Lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser mel processor\n",
    "mel_processor = MelSpectrogramProcessor(\n",
    "    sample_rate=config.audio.sample_rate,\n",
    "    n_fft=config.audio.n_fft,\n",
    "    hop_length=config.audio.hop_length,\n",
    "    win_length=config.audio.win_length,\n",
    "    n_mels=config.audio.n_mels,\n",
    "    fmin=config.audio.fmin,\n",
    "    fmax=config.audio.fmax\n",
    ")\n",
    "\n",
    "# Convertir mels en audio\n",
    "print(\"Converting to audio...\")\n",
    "source_audio = mel_processor.mel_to_wav(source_mel[idx].cpu())\n",
    "target_audio = mel_processor.mel_to_wav(target_mel[idx].cpu())\n",
    "converted_audio = mel_processor.mel_to_wav(converted_mel[idx].cpu())\n",
    "\n",
    "print(\"\\nSource audio (A):\")\n",
    "display(ipd.Audio(source_audio.numpy(), rate=config.audio.sample_rate))\n",
    "\n",
    "print(\"\\nTarget audio (B):\")\n",
    "display(ipd.Audio(target_audio.numpy(), rate=config.audio.sample_rate))\n",
    "\n",
    "print(\"\\nConverted audio (A→B):\")\n",
    "display(ipd.Audio(converted_audio.numpy(), rate=config.audio.sample_rate))\n",
    "\n",
    "print(\"\\n Note: Griffin-Lim produces artifacts. Use neural vocoder for better quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Résumé\n",
    "\n",
    "### Tests Réussis \n",
    "1. Initialisation du modèle\n",
    "2. Forward pass\n",
    "3. Calcul des pertes\n",
    "4. Mini-entraînement\n",
    "5. Génération d'exemples\n",
    "\n",
    "### Observations\n",
    "- Le modèle peut apprendre (pertes décroissantes)\n",
    "- Après 10 epochs, conversion visible mais imparfaite\n",
    "- Besoin de plus d'epochs pour qualité production\n",
    "\n",
    "### Prochaines Étapes\n",
    "1. Entraîner sur dataset complet (50-100 epochs)\n",
    "2. Utiliser vocoder neuronal\n",
    "3. Évaluer avec métriques objectives\n",
    "4. Fine-tuning des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Model training pipeline validated!\")\n",
    "print(\"\\nReady for full training with:\")\n",
    "print(\"  python ../scripts/train.py --data_dir ../data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}