{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook 4: Évaluation du Modèle\n",
    "\n",
    "Ce notebook évalue les performances du modèle VoiceGAN entraîné.\n",
    "\n",
    "**Objectifs:**\n",
    "- Charger un modèle entraîné\n",
    "- Calculer métriques objectives (MCD, similarité)\n",
    "- Visualiser exemples de conversion\n",
    "- Analyser la qualité\n",
    "- Comparaisons qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from config.model_config import Config\n",
    "from src.models.voicegan import VoiceGAN\n",
    "from src.evaluation.metrics import VoiceConversionMetrics\n",
    "from src.preprocessing.audio_processor import AudioProcessor\n",
    "from src.preprocessing.mel_spectrogram import MelSpectrogramProcessor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "CONFIG_PATH = '../config/config.yaml'\n",
    "CHECKPOINT_PATH = '../checkpoints/best_model.pt'  # Modifier selon votre checkpoint\n",
    "TEST_DIR = Path('../data/test')\n",
    "\n",
    "# Charger config\n",
    "config = Config(CONFIG_PATH)\n",
    "\n",
    "print(f\"Config loaded from: {CONFIG_PATH}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Test directory: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Charger le Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, config, device):\n",
    "    \"\"\"Charger modèle depuis checkpoint\"\"\"\n",
    "    model = VoiceGAN(\n",
    "        n_mels=config.audio.n_mels,\n",
    "        content_channels=config.content_encoder.channels,\n",
    "        transformer_dim=config.content_encoder.transformer_dim,\n",
    "        num_heads=config.content_encoder.num_heads,\n",
    "        num_transformer_layers=config.content_encoder.num_layers,\n",
    "        style_dim=config.style_encoder.style_dim\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model, checkpoint = load_model(CHECKPOINT_PATH, config, device)\n",
    "\n",
    "print(f\"\\n Model loaded!\")\n",
    "print(f\"  Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "print(f\"  Global step: {checkpoint.get('global_step', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialiser Processeurs et Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio processors\n",
    "audio_processor = AudioProcessor(\n",
    "    sample_rate=config.audio.sample_rate,\n",
    "    segment_length=config.audio.segment_length\n",
    ")\n",
    "\n",
    "mel_processor = MelSpectrogramProcessor(\n",
    "    sample_rate=config.audio.sample_rate,\n",
    "    n_fft=config.audio.n_fft,\n",
    "    hop_length=config.audio.hop_length,\n",
    "    win_length=config.audio.win_length,\n",
    "    n_mels=config.audio.n_mels,\n",
    "    fmin=config.audio.fmin,\n",
    "    fmax=config.audio.fmax\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "metrics_calculator = VoiceConversionMetrics(\n",
    "    sample_rate=config.audio.sample_rate\n",
    ")\n",
    "\n",
    "print(\" Processors and metrics initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluation sur Dataset de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model, test_dir, num_samples=20):\n",
    "    \"\"\"Évaluer sur un ensemble de test\"\"\"\n",
    "    speakers = [d for d in test_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Pour chaque paire de speakers\n",
    "    for source_speaker in tqdm(speakers[:2], desc=\"Source speakers\"):  # Limiter pour demo\n",
    "        for target_speaker in speakers[:2]:\n",
    "            if source_speaker == target_speaker:\n",
    "                continue\n",
    "            \n",
    "            source_files = list(source_speaker.glob('*.wav'))[:num_samples]\n",
    "            target_files = list(target_speaker.glob('*.wav'))[:num_samples]\n",
    "            \n",
    "            for source_file in source_files[:3]:  # 3 samples par paire\n",
    "                target_file = np.random.choice(target_files)\n",
    "                \n",
    "                try:\n",
    "                    # Charger audio\n",
    "                    source_audio = audio_processor.load_audio(source_file)\n",
    "                    target_audio = audio_processor.load_audio(target_file)\n",
    "                    \n",
    "                    # Segment\n",
    "                    source_audio = audio_processor.segment_audio(source_audio, random=False)\n",
    "                    target_audio = audio_processor.segment_audio(target_audio, random=False)\n",
    "                    \n",
    "                    # Mels\n",
    "                    source_mel = mel_processor.wav_to_mel(source_audio).unsqueeze(0).to(device)\n",
    "                    target_mel = mel_processor.wav_to_mel(target_audio).unsqueeze(0).to(device)\n",
    "                    \n",
    "                    # Convert\n",
    "                    with torch.no_grad():\n",
    "                        converted_mel = model.convert(source_mel, target_mel)\n",
    "                        \n",
    "                        # Extract styles\n",
    "                        converted_style = model.encode_style(converted_mel)\n",
    "                        target_style = model.encode_style(target_mel)\n",
    "                    \n",
    "                    # Calculer métriques\n",
    "                    metrics = metrics_calculator.compute_all_metrics(\n",
    "                        converted_mel.squeeze(0).cpu(),\n",
    "                        target_mel.squeeze(0).cpu(),\n",
    "                        converted_style.squeeze(0).cpu(),\n",
    "                        target_style.squeeze(0).cpu()\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'source_speaker': source_speaker.name,\n",
    "                        'target_speaker': target_speaker.name,\n",
    "                        **metrics\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {source_file}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "results_df = evaluate_test_set(model, TEST_DIR, num_samples=5)\n",
    "\n",
    "print(f\"\\n✅ Evaluated {len(results_df)} samples\")\n",
    "print(\"\\nResults:\")\n",
    "print(results_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser distributions des métriques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['mcd', 'cosine_similarity', 'spectral_convergence', 'log_spectral_distance']\n",
    "titles = ['MCD (lower is better)', 'Cosine Similarity (higher is better)', \n",
    "          'Spectral Convergence (lower is better)', 'Log Spectral Distance (lower is better)']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    axes[i].hist(results_df[metric], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[i].axvline(results_df[metric].mean(), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Mean: {results_df[metric].mean():.3f}')\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résumé statistique\n",
    "print(\"\\n=== METRICS SUMMARY ===\")\n",
    "for metric in metrics_to_plot:\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    print(f\"  Mean: {results_df[metric].mean():.4f}\")\n",
    "    print(f\"  Std: {results_df[metric].std():.4f}\")\n",
    "    print(f\"  Min: {results_df[metric].min():.4f}\")\n",
    "    print(f\"  Max: {results_df[metric].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exemples Visuels de Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conversion(source_file, target_file, model):\n",
    "    \"\"\"Visualiser une conversion A→B\"\"\"\n",
    "    # Charger\n",
    "    source_audio = audio_processor.load_audio(source_file)\n",
    "    target_audio = audio_processor.load_audio(target_file)\n",
    "    \n",
    "    # Segment\n",
    "    source_audio = audio_processor.segment_audio(source_audio, random=False)\n",
    "    target_audio = audio_processor.segment_audio(target_audio, random=False)\n",
    "    \n",
    "    # Mels\n",
    "    source_mel = mel_processor.wav_to_mel(source_audio).unsqueeze(0).to(device)\n",
    "    target_mel = mel_processor.wav_to_mel(target_audio).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Convert\n",
    "    with torch.no_grad():\n",
    "        converted_mel = model.convert(source_mel, target_mel)\n",
    "    \n",
    "    # Convert to audio\n",
    "    converted_audio = mel_processor.mel_to_wav(converted_mel.squeeze(0).cpu())\n",
    "    \n",
    "    # Visualiser\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Source\n",
    "    librosa.display.waveshow(source_audio.numpy(), sr=config.audio.sample_rate, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'Source (A) - Waveform')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    axes[0, 1].imshow(source_mel.squeeze(0).cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0, 1].set_title('Source (A) - Mel-Spectrogram')\n",
    "    axes[0, 1].set_ylabel('Mel Bin')\n",
    "    \n",
    "    # Target\n",
    "    librosa.display.waveshow(target_audio.numpy(), sr=config.audio.sample_rate, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Target (B) - Waveform')\n",
    "    axes[1, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    axes[1, 1].imshow(target_mel.squeeze(0).cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1, 1].set_title('Target (B) - Mel-Spectrogram')\n",
    "    axes[1, 1].set_ylabel('Mel Bin')\n",
    "    \n",
    "    # Converted\n",
    "    librosa.display.waveshow(converted_audio.numpy(), sr=config.audio.sample_rate, ax=axes[2, 0])\n",
    "    axes[2, 0].set_title('Converted (A→B) - Waveform')\n",
    "    axes[2, 0].set_ylabel('Amplitude')\n",
    "    axes[2, 0].set_xlabel('Time (s)')\n",
    "    \n",
    "    axes[2, 1].imshow(converted_mel.squeeze(0).cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[2, 1].set_title('Converted (A→B) - Mel-Spectrogram')\n",
    "    axes[2, 1].set_ylabel('Mel Bin')\n",
    "    axes[2, 1].set_xlabel('Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Audio playback\n",
    "    print(\"\\nListen to:\")\n",
    "    print(\"\\n1. Source (A):\")\n",
    "    display(ipd.Audio(source_audio.numpy(), rate=config.audio.sample_rate))\n",
    "    \n",
    "    print(\"\\n2. Target (B):\")\n",
    "    display(ipd.Audio(target_audio.numpy(), rate=config.audio.sample_rate))\n",
    "    \n",
    "    print(\"\\n3. Converted (A→B):\")\n",
    "    display(ipd.Audio(converted_audio.numpy(), rate=config.audio.sample_rate))\n",
    "    \n",
    "    return source_mel, target_mel, converted_mel\n",
    "\n",
    "# Sélectionner fichiers de test\n",
    "speakers = [d for d in TEST_DIR.iterdir() if d.is_dir()][:2]\n",
    "source_file = list(speakers[0].glob('*.wav'))[0]\n",
    "target_file = list(speakers[1].glob('*.wav'))[0]\n",
    "\n",
    "print(f\"Source: {source_file.name}\")\n",
    "print(f\"Target: {target_file.name}\")\n",
    "print(\"\")\n",
    "\n",
    "source_mel, target_mel, converted_mel = visualize_conversion(source_file, target_file, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse par Paire de Locuteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouper par paire de locuteurs\n",
    "if len(results_df) > 0:\n",
    "    results_df['pair'] = results_df['source_speaker'] + ' → ' + results_df['target_speaker']\n",
    "    \n",
    "    pair_stats = results_df.groupby('pair').agg({\n",
    "        'mcd': ['mean', 'std'],\n",
    "        'cosine_similarity': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n=== PERFORMANCE BY SPEAKER PAIR ===\")\n",
    "    print(pair_stats)\n",
    "    \n",
    "    # Visualiser\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # MCD par paire\n",
    "    pair_mcd = results_df.groupby('pair')['mcd'].mean().sort_values()\n",
    "    axes[0].barh(range(len(pair_mcd)), pair_mcd.values)\n",
    "    axes[0].set_yticks(range(len(pair_mcd)))\n",
    "    axes[0].set_yticklabels(pair_mcd.index)\n",
    "    axes[0].set_xlabel('MCD (lower is better)')\n",
    "    axes[0].set_title('MCD by Speaker Pair')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Similarité par paire\n",
    "    pair_sim = results_df.groupby('pair')['cosine_similarity'].mean().sort_values(ascending=False)\n",
    "    axes[1].barh(range(len(pair_sim)), pair_sim.values, color='orange')\n",
    "    axes[1].set_yticks(range(len(pair_sim)))\n",
    "    axes[1].set_yticklabels(pair_sim.index)\n",
    "    axes[1].set_xlabel('Cosine Similarity (higher is better)')\n",
    "    axes[1].set_title('Style Similarity by Speaker Pair')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interprétation des Résultats\n",
    "\n",
    "### Guides d'Interprétation:\n",
    "\n",
    "**MCD (Mel Cepstral Distortion)**\n",
    "- < 5 dB: Excellent\n",
    "- 5-7 dB: Bon\n",
    "- 7-10 dB: Acceptable\n",
    "- > 10 dB: Nécessite amélioration\n",
    "\n",
    "**Cosine Similarity**\n",
    "- > 0.9: Excellent\n",
    "- 0.8-0.9: Bon\n",
    "- 0.7-0.8: Acceptable\n",
    "- < 0.7: Nécessite amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation qualitative\n",
    "def interpret_results(results_df):\n",
    "    \"\"\"Interpréter les résultats\"\"\"\n",
    "    mean_mcd = results_df['mcd'].mean()\n",
    "    mean_sim = results_df['cosine_similarity'].mean()\n",
    "    \n",
    "    print(\"=== QUALITY ASSESSMENT ===\")\n",
    "    print(f\"\\nMCD: {mean_mcd:.2f} dB\")\n",
    "    if mean_mcd < 5:\n",
    "        print(\"  →  Excellent quality\")\n",
    "    elif mean_mcd < 7:\n",
    "        print(\"  →  Good quality\")\n",
    "    elif mean_mcd < 10:\n",
    "        print(\"  →  Acceptable quality\")\n",
    "    else:\n",
    "        print(\"  →  Needs improvement\")\n",
    "    \n",
    "    print(f\"\\nStyle Similarity: {mean_sim:.3f}\")\n",
    "    if mean_sim > 0.9:\n",
    "        print(\"  →  Excellent style transfer\")\n",
    "    elif mean_sim > 0.8:\n",
    "        print(\"  →  Good style transfer\")\n",
    "    elif mean_sim > 0.7:\n",
    "        print(\"  →  Acceptable style transfer\")\n",
    "    else:\n",
    "        print(\"  →  Style transfer needs improvement\")\n",
    "    \n",
    "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "    if mean_mcd > 7:\n",
    "        print(\"  • Increase lambda_reconstruction\")\n",
    "        print(\"  • Train for more epochs\")\n",
    "    if mean_sim < 0.8:\n",
    "        print(\"  • Increase lambda_identity\")\n",
    "        print(\"  • Verify style encoder learning\")\n",
    "\n",
    "interpret_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder résultats\n",
    "output_dir = Path('../outputs/evaluation')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSV des résultats\n",
    "results_df.to_csv(output_dir / 'evaluation_results.csv', index=False)\n",
    "print(f\"Results saved to: {output_dir / 'evaluation_results.csv'}\")\n",
    "\n",
    "# Statistiques summary\n",
    "summary = {\n",
    "    'num_samples': len(results_df),\n",
    "    'mean_mcd': float(results_df['mcd'].mean()),\n",
    "    'std_mcd': float(results_df['mcd'].std()),\n",
    "    'mean_cosine_similarity': float(results_df['cosine_similarity'].mean()),\n",
    "    'std_cosine_similarity': float(results_df['cosine_similarity'].std()),\n",
    "    'mean_spectral_convergence': float(results_df['spectral_convergence'].mean()),\n",
    "    'mean_log_spectral_distance': float(results_df['log_spectral_distance'].mean())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to: {output_dir / 'summary.json'}\")\n",
    "\n",
    "print(\"\\n Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Résumé de l'Évaluation\n",
    "\n",
    "### Métriques Calculées \n",
    "1. MCD (Mel Cepstral Distortion)\n",
    "2. Cosine Similarity\n",
    "3. Spectral Convergence\n",
    "4. Log Spectral Distance\n",
    "\n",
    "### Analyses Effectuées \n",
    "1. Distribution des métriques\n",
    "2. Performance par paire de locuteurs\n",
    "3. Exemples visuels et auditifs\n",
    "4. Interprétation qualitative\n",
    "\n",
    "### Pour le Rapport\n",
    "- Inclure les graphiques de distribution\n",
    "- Présenter les exemples de conversion\n",
    "- Discuter les résultats par rapport à l'état de l'art\n",
    "- Mentionner les limitations et améliorations possibles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}