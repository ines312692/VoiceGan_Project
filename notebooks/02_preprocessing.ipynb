{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook 2: Prétraitement Audio\n",
    "\n",
    "Ce notebook teste et démontre le pipeline de prétraitement audio.\n",
    "\n",
    "**Objectifs:**\n",
    "- Tester le chargement audio\n",
    "- Conversion audio → mel-spectrogram\n",
    "- Conversion mel-spectrogram → audio (Griffin-Lim)\n",
    "- Visualiser les transformations\n",
    "- Valider la qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from pathlib import Path\n",
    "\n",
    "from src.preprocessing.audio_processor import AudioProcessor\n",
    "from src.preprocessing.mel_spectrogram import MelSpectrogramProcessor\n",
    "from config.model_config import Config\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger configuration\n",
    "config = Config('../config/config.yaml')\n",
    "\n",
    "print(\"Audio Config:\")\n",
    "print(f\"  Sample Rate: {config.audio.sample_rate}\")\n",
    "print(f\"  N-FFT: {config.audio.n_fft}\")\n",
    "print(f\"  Hop Length: {config.audio.hop_length}\")\n",
    "print(f\"  N-Mels: {config.audio.n_mels}\")\n",
    "print(f\"  Segment Length: {config.audio.segment_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialiser les Processeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer processeurs\n",
    "audio_processor = AudioProcessor(\n",
    "    sample_rate=config.audio.sample_rate,\n",
    "    segment_length=config.audio.segment_length,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "mel_processor = MelSpectrogramProcessor(\n",
    "    sample_rate=config.audio.sample_rate,\n",
    "    n_fft=config.audio.n_fft,\n",
    "    hop_length=config.audio.hop_length,\n",
    "    win_length=config.audio.win_length,\n",
    "    n_mels=config.audio.n_mels,\n",
    "    fmin=config.audio.fmin,\n",
    "    fmax=config.audio.fmax\n",
    ")\n",
    "\n",
    "print(\"✅ Processors initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test: Chargement Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver un fichier audio de test\n",
    "DATA_DIR = Path('../data/train')\n",
    "speakers = [d for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "test_file = list(speakers[0].glob('*.wav'))[0]\n",
    "\n",
    "print(f\"Test file: {test_file}\")\n",
    "\n",
    "# Charger audio\n",
    "audio = audio_processor.load_audio(test_file)\n",
    "\n",
    "print(f\"\\nAudio shape: {audio.shape}\")\n",
    "print(f\"Duration: {len(audio) / config.audio.sample_rate:.2f}s\")\n",
    "print(f\"Min value: {audio.min():.4f}\")\n",
    "print(f\"Max value: {audio.max():.4f}\")\n",
    "print(f\"Mean: {audio.mean():.4f}\")\n",
    "print(f\"Std: {audio.std():.4f}\")\n",
    "\n",
    "# Écouter\n",
    "ipd.Audio(audio.numpy(), rate=config.audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test: Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenter audio\n",
    "segment = audio_processor.segment_audio(audio, random=True)\n",
    "\n",
    "print(f\"Original length: {len(audio)}\")\n",
    "print(f\"Segment length: {len(segment)}\")\n",
    "print(f\"Segment duration: {len(segment) / config.audio.sample_rate:.2f}s\")\n",
    "\n",
    "# Visualiser\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "librosa.display.waveshow(audio.numpy(), sr=config.audio.sample_rate, ax=axes[0])\n",
    "axes[0].set_title('Original Audio')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(segment.numpy(), sr=config.audio.sample_rate, ax=axes[1])\n",
    "axes[1].set_title('Segmented Audio')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Écouter segment\n",
    "ipd.Audio(segment.numpy(), rate=config.audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test: Conversion Audio → Mel-Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en mel-spectrogram\n",
    "mel = mel_processor.wav_to_mel(segment)\n",
    "\n",
    "print(f\"Mel-spectrogram shape: {mel.shape}\")\n",
    "print(f\"  N-mels: {mel.shape[0]}\")\n",
    "print(f\"  Time steps: {mel.shape[1]}\")\n",
    "print(f\"Min value: {mel.min():.4f}\")\n",
    "print(f\"Max value: {mel.max():.4f}\")\n",
    "print(f\"Mean: {mel.mean():.4f}\")\n",
    "print(f\"Std: {mel.std():.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.imshow(mel.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel-Spectrogram (Log Scale)')\n",
    "plt.ylabel('Mel Frequency Bin')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test: Conversion Mel-Spectrogram → Audio (Griffin-Lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruire audio depuis mel\n",
    "print(\"Reconstructing audio using Griffin-Lim...\")\n",
    "reconstructed = mel_processor.mel_to_wav(mel)\n",
    "\n",
    "print(f\"\\nReconstructed shape: {reconstructed.shape}\")\n",
    "print(f\"Duration: {len(reconstructed) / config.audio.sample_rate:.2f}s\")\n",
    "\n",
    "# Comparer\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 9))\n",
    "\n",
    "# Original\n",
    "librosa.display.waveshow(segment.numpy(), sr=config.audio.sample_rate, ax=axes[0])\n",
    "axes[0].set_title('Original Audio')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Reconstructed\n",
    "librosa.display.waveshow(reconstructed.numpy(), sr=config.audio.sample_rate, ax=axes[1])\n",
    "axes[1].set_title('Reconstructed Audio (Griffin-Lim)')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "\n",
    "# Différence\n",
    "# Aligner les longueurs\n",
    "min_len = min(len(segment), len(reconstructed))\n",
    "diff = segment[:min_len] - reconstructed[:min_len]\n",
    "librosa.display.waveshow(diff.numpy(), sr=config.audio.sample_rate, ax=axes[2])\n",
    "axes[2].set_title('Difference')\n",
    "axes[2].set_ylabel('Amplitude')\n",
    "axes[2].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculer MSE\n",
    "mse = torch.nn.functional.mse_loss(segment[:min_len], reconstructed[:min_len])\n",
    "print(f\"\\nMSE between original and reconstructed: {mse:.6f}\")\n",
    "\n",
    "print(\"\\nListen to:\")\n",
    "print(\"Original:\")\n",
    "display(ipd.Audio(segment.numpy(), rate=config.audio.sample_rate))\n",
    "print(\"\\nReconstructed:\")\n",
    "display(ipd.Audio(reconstructed.numpy(), rate=config.audio.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test: Pipeline Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_pipeline(audio_path):\n",
    "    \"\"\"Tester le pipeline complet\"\"\"\n",
    "    print(f\"Testing: {audio_path.name}\")\n",
    "    \n",
    "    # 1. Charger\n",
    "    audio = audio_processor.load_audio(audio_path)\n",
    "    print(f\"  Loaded: {audio.shape}\")\n",
    "    \n",
    "    # 2. Segmenter\n",
    "    segment = audio_processor.segment_audio(audio, random=False)\n",
    "    print(f\"  Segmented: {segment.shape}\")\n",
    "    \n",
    "    # 3. Mel\n",
    "    mel = mel_processor.wav_to_mel(segment)\n",
    "    print(f\"  Mel: {mel.shape}\")\n",
    "    \n",
    "    # 4. Reconstruire\n",
    "    reconstructed = mel_processor.mel_to_wav(mel)\n",
    "    print(f\"  Reconstructed: {reconstructed.shape}\")\n",
    "    \n",
    "    # Visualiser\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Waveforms\n",
    "    librosa.display.waveshow(segment.numpy(), sr=config.audio.sample_rate, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Original Waveform')\n",
    "    \n",
    "    librosa.display.waveshow(reconstructed.numpy(), sr=config.audio.sample_rate, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Reconstructed Waveform')\n",
    "    \n",
    "    # Spectrograms\n",
    "    axes[1, 0].imshow(mel.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1, 0].set_title('Mel-Spectrogram')\n",
    "    axes[1, 0].set_ylabel('Mel Bin')\n",
    "    axes[1, 0].set_xlabel('Time')\n",
    "    \n",
    "    # Mel du reconstructed\n",
    "    mel_reconstructed = mel_processor.wav_to_mel(reconstructed)\n",
    "    axes[1, 1].imshow(mel_reconstructed.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1, 1].set_title('Mel-Spectrogram (Reconstructed)')\n",
    "    axes[1, 1].set_ylabel('Mel Bin')\n",
    "    axes[1, 1].set_xlabel('Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return segment, mel, reconstructed\n",
    "\n",
    "# Tester sur plusieurs fichiers\n",
    "test_files = list(speakers[0].glob('*.wav'))[:2]\n",
    "\n",
    "for test_file in test_files:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    segment, mel, reconstructed = test_full_pipeline(test_file)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test: Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger plusieurs fichiers pour calculer stats\n",
    "mel_list = []\n",
    "for audio_file in list(speakers[0].glob('*.wav'))[:20]:\n",
    "    audio = audio_processor.load_audio(audio_file)\n",
    "    segment = audio_processor.segment_audio(audio)\n",
    "    mel = mel_processor.wav_to_mel(segment)\n",
    "    mel_list.append(mel)\n",
    "\n",
    "# Calculer stats\n",
    "mean, std = mel_processor.compute_stats(mel_list)\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"  Mean: {mean:.4f}\")\n",
    "print(f\"  Std: {std:.4f}\")\n",
    "\n",
    "# Normaliser\n",
    "mel_normalized = mel_processor.normalize(mel_list[0], mean, std)\n",
    "\n",
    "print(f\"\\nOriginal mel:\")\n",
    "print(f\"  Min: {mel_list[0].min():.4f}\")\n",
    "print(f\"  Max: {mel_list[0].max():.4f}\")\n",
    "print(f\"  Mean: {mel_list[0].mean():.4f}\")\n",
    "print(f\"  Std: {mel_list[0].std():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized mel:\")\n",
    "print(f\"  Min: {mel_normalized.min():.4f}\")\n",
    "print(f\"  Max: {mel_normalized.max():.4f}\")\n",
    "print(f\"  Mean: {mel_normalized.mean():.4f}\")\n",
    "print(f\"  Std: {mel_normalized.std():.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "im0 = axes[0].imshow(mel_list[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title('Original Mel-Spectrogram')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(mel_normalized.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title('Normalized Mel-Spectrogram')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler un batch\n",
    "batch_size = 4\n",
    "batch_audio = []\n",
    "batch_mels = []\n",
    "\n",
    "for audio_file in list(speakers[0].glob('*.wav'))[:batch_size]:\n",
    "    audio = audio_processor.load_audio(audio_file)\n",
    "    segment = audio_processor.segment_audio(audio)\n",
    "    mel = mel_processor.wav_to_mel(segment)\n",
    "    \n",
    "    batch_audio.append(segment)\n",
    "    batch_mels.append(mel)\n",
    "\n",
    "# Stack en batch\n",
    "batch_audio_tensor = torch.stack(batch_audio)\n",
    "batch_mels_tensor = torch.stack(batch_mels)\n",
    "\n",
    "print(f\"Batch audio shape: {batch_audio_tensor.shape}\")\n",
    "print(f\"Batch mels shape: {batch_mels_tensor.shape}\")\n",
    "\n",
    "# Visualiser batch\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    axes[i].imshow(batch_mels_tensor[i].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i].set_title(f'Sample {i+1}')\n",
    "    axes[i].set_ylabel('Mel Bin')\n",
    "    axes[i].set_xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Résumé & Validation\n",
    "\n",
    "### Tests Réussis \n",
    "1. Chargement audio\n",
    "2. Segmentation\n",
    "3. Conversion audio → mel\n",
    "4. Conversion mel → audio\n",
    "5. Normalisation\n",
    "6. Batch processing\n",
    "\n",
    "### Observations\n",
    "- La reconstruction avec Griffin-Lim introduit des artefacts (normal)\n",
    "- Pour la production, un vocoder neuronal (HiFi-GAN) sera meilleur\n",
    "- La normalisation aide à stabiliser l'entraînement\n",
    "\n",
    "### Prochaines Étapes\n",
    "1. Tester avec le dataset complet\n",
    "2. Intégrer dans le DataLoader\n",
    "3. Entraîner le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Preprocessing pipeline validated!\")\n",
    "print(\"\\nReady for training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}